{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class DAN(nn.Module):\n",
    "    def __init__(self, num_class=7,num_head=4, pretrained=True):\n",
    "        super(DAN, self).__init__()\n",
    "        \n",
    "        resnet = models.resnet18(pretrained)\n",
    "        \n",
    "        if pretrained:\n",
    "            checkpoint = torch.load('./models/resnet18_msceleb.pth')\n",
    "            resnet.load_state_dict(checkpoint['state_dict'],strict=True)\n",
    "\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.num_head = num_head\n",
    "        for i in range(num_head):\n",
    "            setattr(self,\"cat_head%d\" %i, CrossAttentionHead())\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(512, num_class)\n",
    "        self.bn = nn.BatchNorm1d(num_class)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        heads = []\n",
    "        for i in range(self.num_head):\n",
    "            heads.append(getattr(self,\"cat_head%d\" %i)(x))\n",
    "        \n",
    "        heads = torch.stack(heads).permute([1,0,2])\n",
    "        if heads.size(1)>1:\n",
    "            heads = F.log_softmax(heads,dim=1)\n",
    "            \n",
    "        out = self.fc(heads.sum(dim=1))\n",
    "        out = self.bn(out)\n",
    "   \n",
    "        return out, x, heads\n",
    "\n",
    "class CrossAttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa = SpatialAttention()\n",
    "        self.ca = ChannelAttention()\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "    def forward(self, x):\n",
    "        sa = self.sa(x)\n",
    "        ca = self.ca(sa)\n",
    "\n",
    "        return ca\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "        self.conv_3x3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.conv_1x3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=(1,3),padding=(0,1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.conv_3x1 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=(3,1),padding=(1,0)),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1x1(x)\n",
    "        y = self.relu(self.conv_3x3(y) + self.conv_1x3(y) + self.conv_3x1(y))\n",
    "        y = y.sum(dim=1,keepdim=True) \n",
    "        out = x*y\n",
    "        \n",
    "        return out \n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(512, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 512),\n",
    "            nn.Sigmoid()    \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, sa):\n",
    "        sa = self.gap(sa)\n",
    "        sa = sa.view(sa.size(0),-1)\n",
    "        y = self.attention(sa)\n",
    "        out = sa * y\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--image IMAGE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9008 --control=9006 --hb=9005 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"880ea5e1-324e-46c2-be37-d9bd5b657955\" --shell=9007 --transport=\"tcp\" --iopub=9009 --f=c:\\Users\\Balaji_j\\AppData\\Roaming\\jupyter\\runtime\\kernel-16860fnv9buAGsKi4.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Balaji_j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3405: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--image', type=str, help='Image file for evaluation.')\n",
    " \n",
    "    return parser.parse_args()\n",
    "\n",
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.data_transforms = transforms.Compose([\n",
    "                                    transforms.Resize((224, 224)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "        self.labels = ['neutral', 'happy', 'sad', 'surprise', 'fear', 'disgust', 'anger']\n",
    "\n",
    "        self.model = DAN(num_head=4, num_class=7, pretrained=False)\n",
    "        checkpoint = torch.load(r'affecnet7_epoch6_acc0.6569.pth',\n",
    "            map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    def detect(self, img0):\n",
    "        img = cv2.cvtColor(np.asarray(img0),cv2.COLOR_RGB2BGR)\n",
    "        faces = self.face_cascade.detectMultiScale(img)\n",
    "        \n",
    "        return faces\n",
    "\n",
    "    def fer(self, path):\n",
    "\n",
    "        img0 = Image.open(path).convert('RGB')\n",
    "\n",
    "        faces = self.detect(img0)\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            return 'null'\n",
    "\n",
    "        ##  single face detection\n",
    "        labels = []\n",
    "        for x, y, w, h in faces:\n",
    "\n",
    "        \n",
    "            img = img0.crop((x,y, x+w, y+h))\n",
    "\n",
    "            img = self.data_transforms(img)\n",
    "            img = img.view(1,3,224,224)\n",
    "            img = img.to(self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                out, _, _ = self.model(img)\n",
    "                _, pred = torch.max(out,1)\n",
    "                index = int(pred)\n",
    "                label = self.labels[index]\n",
    "                labels.append(label)\n",
    "        return labels,faces\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "\n",
    "    model = Model()\n",
    "\n",
    "    image = r\"E:\\Downloads\\children_grp.jpg\"\n",
    "\n",
    "    #assert os.path.exists(image), \"Failed to load image file.\"\n",
    "\n",
    "    labels,faces = model.fer(image)\n",
    "\n",
    "    if len(faces)>0:\n",
    "        for idx,(x,y,w,h) in enumerate(faces):\n",
    "            label_position = (x,y)\n",
    "            cv2.rectangle(image,(x,y),(x+w,y+h),(0,145,255),1)\n",
    "            cv2.putText(image,labels[idx],label_position,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "        cv2.imshow('Emotion Detector',image)\n",
    "        \n",
    "    else:\n",
    "        cv2.putText(image,'No Faces',(179,59),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "        cv2.imshow('Emotion Detector',image)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(arr):\n",
    "                    return max(set(arr.tolist()), key = list.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let' 'acham' 'veera' 'loosu' 'thandam']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "descriptor 'count' for 'list' objects doesn't apply to a 'str' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Balaji_j\\atum_prj\\face_expression_classifier\\face_exp_classifier3.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Balaji_j/atum_prj/face_expression_classifier/face_exp_classifier3.ipynb#ch0000002?line=7'>8</a>\u001b[0m npar\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(ls)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Balaji_j/atum_prj/face_expression_classifier/face_exp_classifier3.ipynb#ch0000002?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(npar[:,\u001b[39m0\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Balaji_j/atum_prj/face_expression_classifier/face_exp_classifier3.ipynb#ch0000002?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(most_frequent(npar[:,\u001b[39m0\u001b[39;49m]))\n",
      "\u001b[1;32mc:\\Users\\Balaji_j\\atum_prj\\face_expression_classifier\\face_exp_classifier3.ipynb Cell 3'\u001b[0m in \u001b[0;36mmost_frequent\u001b[1;34m(arr)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Balaji_j/atum_prj/face_expression_classifier/face_exp_classifier3.ipynb#ch0000004?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmost_frequent\u001b[39m(arr):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Balaji_j/atum_prj/face_expression_classifier/face_exp_classifier3.ipynb#ch0000004?line=1'>2</a>\u001b[0m                     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39;49m(\u001b[39mset\u001b[39;49m(arr\u001b[39m.\u001b[39;49mtolist()), key \u001b[39m=\u001b[39;49m \u001b[39mlist\u001b[39;49m\u001b[39m.\u001b[39;49mcount)\n",
      "\u001b[1;31mTypeError\u001b[0m: descriptor 'count' for 'list' objects doesn't apply to a 'str' object"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ls = [['let','us'],['acham','madamaiyada'],['veera','thoranthara'],['loosu','payalae'],['thandam','sooru']]\n",
    "\n",
    "npar= np.array(ls)\n",
    "print(npar[:,0])\n",
    "print(most_frequent(npar[:,0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f86bd2566df633de3433ebe1e2d10776a9b4d657b73edceb2d78e2ba4453e4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
