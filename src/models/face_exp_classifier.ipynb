{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://github.com/yaoing/DAN\n",
    "# #imports\n",
    "import os\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAN ARCHIETURE \n",
    "\n",
    "class DAN(nn.Module):\n",
    "    def __init__(self, num_class=7,num_head=4, pretrained=True):\n",
    "        super(DAN, self).__init__()\n",
    "        \n",
    "        resnet = models.resnet18(pretrained)\n",
    "        \n",
    "        if pretrained:\n",
    "            checkpoint = torch.load('./models/resnet18_msceleb.pth')\n",
    "            resnet.load_state_dict(checkpoint['state_dict'],strict=True)\n",
    "\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.num_head = num_head\n",
    "        for i in range(num_head):\n",
    "            setattr(self,\"cat_head%d\" %i, CrossAttentionHead())\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(512, num_class)\n",
    "        self.bn = nn.BatchNorm1d(num_class)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        heads = []\n",
    "        for i in range(self.num_head):\n",
    "            heads.append(getattr(self,\"cat_head%d\" %i)(x))\n",
    "        \n",
    "        heads = torch.stack(heads).permute([1,0,2])\n",
    "        if heads.size(1)>1:\n",
    "            heads = F.log_softmax(heads,dim=1)\n",
    "            \n",
    "        out = self.fc(heads.sum(dim=1))\n",
    "        out = self.bn(out)\n",
    "   \n",
    "        return out, x, heads\n",
    "\n",
    "class CrossAttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa = SpatialAttention()\n",
    "        self.ca = ChannelAttention()\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "    def forward(self, x):\n",
    "        sa = self.sa(x)\n",
    "        ca = self.ca(sa)\n",
    "\n",
    "        return ca\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "        self.conv_3x3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.conv_1x3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=(1,3),padding=(0,1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.conv_3x1 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=(3,1),padding=(1,0)),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1x1(x)\n",
    "        y = self.relu(self.conv_3x3(y) + self.conv_1x3(y) + self.conv_3x1(y))\n",
    "        y = y.sum(dim=1,keepdim=True) \n",
    "        out = x*y\n",
    "        \n",
    "        return out \n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(512, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 512),\n",
    "            nn.Sigmoid()    \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, sa):\n",
    "        sa = self.gap(sa)\n",
    "        sa = sa.view(sa.size(0),-1)\n",
    "        y = self.attention(sa)\n",
    "        out = sa * y\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.data_transforms = transforms.Compose([\n",
    "                                    transforms.Resize((224, 224)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "        self.labels = ['neutral', 'happy', 'sad', 'surprise', 'fear', 'disgust', 'anger']\n",
    "\n",
    "        self.model = DAN(num_head=4, num_class=7, pretrained=False)\n",
    "        checkpoint = torch.load(r\"affecnet7_epoch6_acc0.6569.pth\",\n",
    "            map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    def detect(self, img0):\n",
    "        img = cv2.cvtColor(np.asarray(img0),cv2.COLOR_RGB2BGR)\n",
    "        faces = self.face_cascade.detectMultiScale(img,minNeighbors = 10,minSize=(60,60))\n",
    "    \n",
    "        return faces\n",
    "\n",
    "    def fer(self, frame):\n",
    "\n",
    "        img0 = Image.fromarray(frame)\n",
    "\n",
    "        faces = self.detect(img0)\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            return [],[]\n",
    "\n",
    "        #  single face detection\n",
    "        labels = []\n",
    "        \n",
    "        for (x,y,w,h) in faces:\n",
    "             img = img0.crop((x,y, x+w, y+h))\n",
    "\n",
    "             img = self.data_transforms(img)\n",
    "             img = img.view(1,3,224,224)\n",
    "             img = img.to(self.device)\n",
    "            \n",
    "             with torch.set_grad_enabled(False):\n",
    "                 out, _, _ = self.model(img)\n",
    "                 _, pred = torch.max(out,1)\n",
    "                 index = int(pred)\n",
    "                 label = self.labels[index]\n",
    "                 labels.append(label)\n",
    "        return labels,faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List):\n",
    "    \n",
    "    if len(List) != 0:\n",
    "        counter = 0\n",
    "        value = List[0]\n",
    "     \n",
    "        for i in List:\n",
    "            curr_frequency = List.count(i)\n",
    "            if(curr_frequency> counter):\n",
    "                counter = curr_frequency\n",
    "                value = i\n",
    "    \n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def make_bounding_box(filtered_faces,filtered_labels,max_appeared_label,frame):\n",
    "    for per_person in range(max_appeared_label):\n",
    "            per_person_face = filtered_faces[:,per_person]\n",
    "            per_person_emotion = filtered_labels[:,per_person]\n",
    "        \n",
    "            final_label = most_frequent([i for i in per_person_emotion])\n",
    "            x,y,w,h = per_person_face[np.where(per_person_emotion == final_label)][0]\n",
    "            \n",
    "            label_position = (x,y)\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(0,145,255),1)\n",
    "            cv2.putText(frame,final_label,label_position,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN2022-07-09 12:22:32.586254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Balaji_j\\atum_prj\\Face_emotion_recognition\\face_exp_classifier.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Balaji_j/atum_prj/Face_emotion_recognition/face_exp_classifier.ipynb#ch0000004?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIN\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(datetime\u001b[39m.\u001b[39mnow()))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Balaji_j/atum_prj/Face_emotion_recognition/face_exp_classifier.ipynb#ch0000004?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m frm_of_set \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m30\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Balaji_j/atum_prj/Face_emotion_recognition/face_exp_classifier.ipynb#ch0000004?line=29'>30</a>\u001b[0m     val, frame \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Balaji_j/atum_prj/Face_emotion_recognition/face_exp_classifier.ipynb#ch0000004?line=30'>31</a>\u001b[0m     \u001b[39mif\u001b[39;00m val :\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Balaji_j/atum_prj/Face_emotion_recognition/face_exp_classifier.ipynb#ch0000004?line=31'>32</a>\u001b[0m         labels,faces \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfer(frame)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model = Model()\n",
    "\n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    \n",
    "    fps = 0\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    \n",
    "\n",
    "    while cap.isOpened():\n",
    "            \n",
    "            # print(\"completed 30 frames\"+str(datetime.now()))\n",
    "\n",
    "            tot_labels_set = []\n",
    "            tot_faces_set = []\n",
    "\n",
    "            frame = None\n",
    "\n",
    "            val = True\n",
    "            \n",
    "            print(\"IN\"+str(datetime.now()))\n",
    "            for frm_of_set in range(30):\n",
    "                val, frame = cap.read()\n",
    "                if val :\n",
    "                    labels,faces = model.fer(frame)\n",
    "\n",
    "                    if labels != []:\n",
    "                        tot_labels_set.append(np.array(labels))\n",
    "                        tot_faces_set.append(np.array(faces))\n",
    "            \n",
    "\n",
    "            if val :\n",
    "                    tot_faces_set = np.array(tot_faces_set)\n",
    "                    tot_labels_set = np.array(tot_labels_set)\n",
    "\n",
    "                    max_appeared_label = most_frequent([len(i) for i in tot_labels_set ])\n",
    "\n",
    "                    #filtered according to the max no appeared faces found in each frame to filter \n",
    "                    filtered_indexes = [index for index in range(len(tot_labels_set)) if len(tot_labels_set[index])==max_appeared_label]\n",
    "\n",
    "                    filtered_labels= np.array([tot_labels_set[index] for index in filtered_indexes ])\n",
    "                    filtered_faces = np.array([tot_faces_set[index] for index in filtered_indexes ])\n",
    "\n",
    "                    print(\"OUT\"+str(datetime.now()))\n",
    "                    if max_appeared_label >= 1:  \n",
    "                    \n",
    "                        make_bounding_box(filtered_faces,filtered_labels,max_appeared_label,frame)\n",
    "\n",
    "                        cv2.imshow('Emotion Detector',frame)\n",
    "                        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                            break\n",
    "                        else:\n",
    "                            count = count+1\n",
    "                            continue\n",
    "                        \n",
    "                    else:\n",
    "\n",
    "                        cv2.putText(frame,'No Faces',(179,59),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "                        cv2.imshow('Emotion Detector',frame)\n",
    "                        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                            break\n",
    "                        else: \n",
    "                            count = count+1\n",
    "                            continue\n",
    "            else:\n",
    "                break        \n",
    "                        \n",
    "                        \n",
    "    \n",
    "    print(count)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()     \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "   \n",
    "# # Create a VideoCapture object and read from input file\n",
    "# cap = cv2.VideoCapture(r\"C:\\Users\\Balaji_j\\Downloads\\istockphoto-1288868953-640_adpp_is.mp4\")\n",
    "   \n",
    "# # Check if camera opened successfully\n",
    "# if (cap.isOpened()== False): \n",
    "#   print(\"Error opening video  file\")\n",
    "\n",
    "# print(\"IN\"+str(datetime.now()))\n",
    "   \n",
    "# # Read until video is completed\n",
    "# while (cap.isOpened()):\n",
    "      \n",
    "#   # Capture frame-by-frame\n",
    "#   ret, frame = cap.read()\n",
    "#   if ret == True:\n",
    "   \n",
    "#     # Display the resulting frame\n",
    "#     cv2.imshow('Frame', frame)\n",
    "   \n",
    "#     # Press Q on keyboard to  exit\n",
    "#     if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "#       break\n",
    "   \n",
    "#   # Break the loop\n",
    "#   else: \n",
    "#     break\n",
    "   \n",
    "# # When everything done, release \n",
    "# # the video capture object\n",
    "# cap.release()\n",
    "# print(\"OUT\"+str(datetime.now()))\n",
    "   \n",
    "# # Closes all the frames\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f86bd2566df633de3433ebe1e2d10776a9b4d657b73edceb2d78e2ba4453e4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
