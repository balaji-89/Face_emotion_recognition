{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://github.com/yaoing/DAN\n",
    "# #imports\n",
    "import os\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAN ARCHIETURE \n",
    "\n",
    "class DAN(nn.Module):\n",
    "    def __init__(self, num_class=7,num_head=4, pretrained=True):\n",
    "        super(DAN, self).__init__()\n",
    "        \n",
    "        resnet = models.resnet18(pretrained)\n",
    "        \n",
    "        if pretrained:\n",
    "            checkpoint = torch.load('./models/resnet18_msceleb.pth')\n",
    "            resnet.load_state_dict(checkpoint['state_dict'],strict=True)\n",
    "\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.num_head = num_head\n",
    "        for i in range(num_head):\n",
    "            setattr(self,\"cat_head%d\" %i, CrossAttentionHead())\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(512, num_class)\n",
    "        self.bn = nn.BatchNorm1d(num_class)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        heads = []\n",
    "        for i in range(self.num_head):\n",
    "            heads.append(getattr(self,\"cat_head%d\" %i)(x))\n",
    "        \n",
    "        heads = torch.stack(heads).permute([1,0,2])\n",
    "        if heads.size(1)>1:\n",
    "            heads = F.log_softmax(heads,dim=1)\n",
    "            \n",
    "        out = self.fc(heads.sum(dim=1))\n",
    "        out = self.bn(out)\n",
    "   \n",
    "        return out, x, heads\n",
    "\n",
    "class CrossAttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa = SpatialAttention()\n",
    "        self.ca = ChannelAttention()\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "    def forward(self, x):\n",
    "        sa = self.sa(x)\n",
    "        ca = self.ca(sa)\n",
    "\n",
    "        return ca\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "        self.conv_3x3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.conv_1x3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=(1,3),padding=(0,1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.conv_3x1 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=(3,1),padding=(1,0)),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1x1(x)\n",
    "        y = self.relu(self.conv_3x3(y) + self.conv_1x3(y) + self.conv_3x1(y))\n",
    "        y = y.sum(dim=1,keepdim=True) \n",
    "        out = x*y\n",
    "        \n",
    "        return out \n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(512, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 512),\n",
    "            nn.Sigmoid()    \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, sa):\n",
    "        sa = self.gap(sa)\n",
    "        sa = sa.view(sa.size(0),-1)\n",
    "        y = self.attention(sa)\n",
    "        out = sa * y\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.data_transforms = transforms.Compose([\n",
    "                                    transforms.Resize((224, 224)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "        self.labels = ['neutral', 'happy', 'sad', 'surprise', 'fear', 'disgust', 'anger']\n",
    "\n",
    "        self.model = DAN(num_head=4, num_class=7, pretrained=False)\n",
    "        checkpoint = torch.load(r\"affecnet7_epoch6_acc0.6569.pth\",\n",
    "            map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    def detect(self, img0):\n",
    "        img = cv2.cvtColor(np.asarray(img0),cv2.COLOR_RGB2BGR)\n",
    "        faces = self.face_cascade.detectMultiScale(img,minNeighbors = 10,minSize=(60,60))\n",
    "    \n",
    "        return faces\n",
    "\n",
    "    def fer(self, frame):\n",
    "\n",
    "        img0 = Image.fromarray(frame)\n",
    "\n",
    "        faces = self.detect(img0)\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            return [],[]\n",
    "\n",
    "        #  single face detection\n",
    "        labels = []\n",
    "        \n",
    "        for (x,y,w,h) in faces:\n",
    "             img = img0.crop((x,y, x+w, y+h))\n",
    "\n",
    "             img = self.data_transforms(img)\n",
    "             img = img.view(1,3,224,224)\n",
    "             img = img.to(self.device)\n",
    "            \n",
    "             with torch.set_grad_enabled(False):\n",
    "                 out, _, _ = self.model(img)\n",
    "                 _, pred = torch.max(out,1)\n",
    "                 index = int(pred)\n",
    "                 label = self.labels[index]\n",
    "                 labels.append(label)\n",
    "        return labels,faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List):\n",
    "    \n",
    "    if len(List) != 0:\n",
    "        counter = 0\n",
    "        value = List[0]\n",
    "     \n",
    "        for i in List:\n",
    "            curr_frequency = List.count(i)\n",
    "            if(curr_frequency> counter):\n",
    "                counter = curr_frequency\n",
    "                value = i\n",
    "    \n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def make_bounding_box(filtered_faces,filtered_labels,max_appeared_label,frame):\n",
    "    for per_person in range(max_appeared_label):\n",
    "            per_person_face = filtered_faces[:,per_person]\n",
    "            per_person_emotion = filtered_labels[:,per_person]\n",
    "        \n",
    "            final_label = most_frequent([i for i in per_person_emotion])\n",
    "            x,y,w,h = per_person_face[np.where(per_person_emotion == final_label)][0]\n",
    "            \n",
    "            label_position = (x,y)\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(0,145,255),1)\n",
    "            cv2.putText(frame,final_label,label_position,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Balaji_j\\AppData\\Local\\Temp\\ipykernel_8624\\2446352931.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tot_faces_set = np.array(tot_faces_set)\n",
      "C:\\Users\\Balaji_j\\AppData\\Local\\Temp\\ipykernel_8624\\2446352931.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tot_labels_set = np.array(tot_labels_set)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model = Model()\n",
    "\n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    fps = 0\n",
    "    while True:\n",
    "            tot_labels_set = []\n",
    "            tot_faces_set = []\n",
    "\n",
    "            frame = None\n",
    "\n",
    "            for frm_of_set in range(10):\n",
    "                _, frame = cap.read()\n",
    "                labels,faces = model.fer(frame)\n",
    "                \n",
    "                if labels != []:\n",
    "                    tot_labels_set.append(np.array(labels))\n",
    "                    tot_faces_set.append(np.array(faces))\n",
    "\n",
    "            tot_faces_set = np.array(tot_faces_set)\n",
    "            tot_labels_set = np.array(tot_labels_set)\n",
    "\n",
    "            max_appeared_label = most_frequent([len(i) for i in tot_labels_set ])\n",
    "\n",
    "            #filtered according to the max no appeared faces found in each frame to filter \n",
    "            filtered_indexes = [index for index in range(len(tot_labels_set)) if len(tot_labels_set[index])==max_appeared_label]\n",
    "            \n",
    "            filtered_labels= np.array([tot_labels_set[index] for index in filtered_indexes ])\n",
    "            filtered_faces = np.array([tot_faces_set[index] for index in filtered_indexes ])\n",
    "\n",
    "    \n",
    "            if max_appeared_label >= 1:  \n",
    "\n",
    "                make_bounding_box(filtered_faces,filtered_labels,max_appeared_label,frame)\n",
    "\n",
    "                cv2.imshow('Emotion Detector',frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            else:\n",
    "                \n",
    "                cv2.putText(frame,'No Faces',(179,59),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "                cv2.imshow('Emotion Detector',frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                else: \n",
    "                    continue\n",
    "                \n",
    "\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()     \n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f86bd2566df633de3433ebe1e2d10776a9b4d657b73edceb2d78e2ba4453e4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
