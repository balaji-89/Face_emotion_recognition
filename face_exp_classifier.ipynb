{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://github.com/yaoing/DAN\n",
    "# #imports\n",
    "import os\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAN ARCHIETURE \n",
    "\n",
    "class DAN(nn.Module):\n",
    "    def __init__(self, num_class=7,num_head=4, pretrained=True):\n",
    "        super(DAN, self).__init__()\n",
    "        \n",
    "        resnet = models.resnet18(pretrained)\n",
    "        \n",
    "        if pretrained:\n",
    "            checkpoint = torch.load('./models/resnet18_msceleb.pth')\n",
    "            resnet.load_state_dict(checkpoint['state_dict'],strict=True)\n",
    "\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.num_head = num_head\n",
    "        for i in range(num_head):\n",
    "            setattr(self,\"cat_head%d\" %i, CrossAttentionHead())\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(512, num_class)\n",
    "        self.bn = nn.BatchNorm1d(num_class)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        heads = []\n",
    "        for i in range(self.num_head):\n",
    "            heads.append(getattr(self,\"cat_head%d\" %i)(x))\n",
    "        \n",
    "        heads = torch.stack(heads).permute([1,0,2])\n",
    "        if heads.size(1)>1:\n",
    "            heads = F.log_softmax(heads,dim=1)\n",
    "            \n",
    "        out = self.fc(heads.sum(dim=1))\n",
    "        out = self.bn(out)\n",
    "   \n",
    "        return out, x, heads\n",
    "\n",
    "class CrossAttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa = SpatialAttention()\n",
    "        self.ca = ChannelAttention()\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "    def forward(self, x):\n",
    "        sa = self.sa(x)\n",
    "        ca = self.ca(sa)\n",
    "\n",
    "        return ca\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "        self.conv_3x3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.conv_1x3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=(1,3),padding=(0,1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.conv_3x1 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=(3,1),padding=(1,0)),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1x1(x)\n",
    "        y = self.relu(self.conv_3x3(y) + self.conv_1x3(y) + self.conv_3x1(y))\n",
    "        y = y.sum(dim=1,keepdim=True) \n",
    "        out = x*y\n",
    "        \n",
    "        return out \n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(512, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 512),\n",
    "            nn.Sigmoid()    \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, sa):\n",
    "        sa = self.gap(sa)\n",
    "        sa = sa.view(sa.size(0),-1)\n",
    "        y = self.attention(sa)\n",
    "        out = sa * y\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.data_transforms = transforms.Compose([\n",
    "                                    transforms.Resize((224, 224)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "        self.labels = ['neutral', 'happy', 'sad', 'surprise', 'fear', 'disgust', 'anger']\n",
    "\n",
    "        self.model = DAN(num_head=4, num_class=7, pretrained=False)\n",
    "        checkpoint = torch.load(r\"affecnet7_epoch6_acc0.6569.pth\",\n",
    "            map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    def detect(self, img0):\n",
    "        img = cv2.cvtColor(np.asarray(img0),cv2.COLOR_RGB2BGR)\n",
    "        faces = self.face_cascade.detectMultiScale(img,minNeighbors = 10,minSize=(60,60))\n",
    "    \n",
    "        return faces\n",
    "\n",
    "    def fer(self, frame):\n",
    "\n",
    "        img0 = Image.fromarray(frame)\n",
    "\n",
    "        faces = self.detect(img0)\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            return [],[]\n",
    "\n",
    "        #  single face detection\n",
    "        labels = []\n",
    "        \n",
    "        for (x,y,w,h) in faces:\n",
    "             img = img0.crop((x,y, x+w, y+h))\n",
    "\n",
    "             img = self.data_transforms(img)\n",
    "             img = img.view(1,3,224,224)\n",
    "             img = img.to(self.device)\n",
    "            \n",
    "             with torch.set_grad_enabled(False):\n",
    "                 out, _, _ = self.model(img)\n",
    "                 _, pred = torch.max(out,1)\n",
    "                 index = int(pred)\n",
    "                 label = self.labels[index]\n",
    "                 labels.append(label)\n",
    "        return labels,faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List):\n",
    "    return max(set(List), key = List.count)\n",
    "\n",
    "def get_no_faces(arr):\n",
    "    try:\n",
    "        return arr.shape[1]\n",
    "    except IndexError:\n",
    "        return 1\n",
    "\n",
    "def most_frequent(List):\n",
    "    counter = 0\n",
    "    value = List[0]\n",
    "     \n",
    "    for i in List:\n",
    "        curr_frequency = List.count(i)\n",
    "        if(curr_frequency> counter):\n",
    "            counter = curr_frequency\n",
    "            value = i\n",
    " \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model = Model()\n",
    "\n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    fps = 0\n",
    "    while True:\n",
    "            tot_labels_set = []\n",
    "            tot_faces_set = []\n",
    "\n",
    "            fin_faces = []\n",
    "            fin_labels = []\n",
    "\n",
    "            for frm_of_set in range(20):\n",
    "                _, frame = cap.read()\n",
    "                labels,faces = model.fer(frame)\n",
    "                if labels != []:\n",
    "                    tot_labels_set.append(np.array(labels))\n",
    "                    tot_faces_set.append(np.array(faces))\n",
    "            tot_faces_set = np.array(tot_faces_set)\n",
    "            tot_labels_set = np.array(tot_labels_set)\n",
    "\n",
    "            \n",
    "            if (tot_labels_set[1].shape)[0] == 1:\n",
    "                fin_labels = most_frequent([i[0] for i in tot_labels_set if len(i)>0 ])\n",
    "                fin_faces = tot_faces_set[np.where(tot_labels_set == [fin_labels])][0]\n",
    "                label_position = (fin_faces[0],fin_faces[1])\n",
    "                cv2.rectangle(frame,(fin_faces[0],fin_faces[1]),(fin_faces[0]+fin_faces[2],fin_faces[1]+fin_faces[3]),(0,145,255),1)\n",
    "                cv2.putText(frame,fin_labels,label_position,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "                cv2.imshow('Emotion Detector',frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            break\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "            # if len(faces) >0:\n",
    "            #     for idx,(x,y,w,h) in enumerate(faces):\n",
    "                \n",
    "            #         label_position = (x,y)\n",
    "            #         cv2.rectangle(frame,(x,y),(x+w,y+h),(0,145,255),1)\n",
    "            #         cv2.putText(frame,labels[idx],label_position,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "\n",
    "            #     cv2.imshow('Emotion Detector',frame)\n",
    "            #     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #         break\n",
    "            # else:\n",
    "            #     cv2.putText(frame,'No Faces',(179,59),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "            #     cv2.imshow('Emotion Detector',frame)\n",
    "            #     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #         break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()     \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# print(tot_faces_set)\n",
    "\n",
    "ls = [i.shape[0] for i in tot_faces_set]\n",
    "print(max(set(ls), key = ls.count))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['fear'], dtype='<U4') array(['fear'], dtype='<U4')\n",
      " array(['neutral'], dtype='<U7') array(['neutral', 'anger'], dtype='<U7')\n",
      " array(['neutral', 'anger'], dtype='<U7')\n",
      " array(['neutral', 'neutral'], dtype='<U7')\n",
      " array(['neutral', 'anger'], dtype='<U7')\n",
      " array(['neutral', 'neutral'], dtype='<U7')\n",
      " array(['neutral', 'anger'], dtype='<U7') array(['fear'], dtype='<U4')\n",
      " array(['neutral', 'neutral'], dtype='<U7')\n",
      " array(['neutral', 'anger'], dtype='<U7')\n",
      " array(['neutral', 'neutral'], dtype='<U7')\n",
      " array(['neutral', 'anger'], dtype='<U7')\n",
      " array(['neutral', 'neutral'], dtype='<U7')\n",
      " array(['neutral', 'neutral'], dtype='<U7')\n",
      " array(['neutral', 'neutral'], dtype='<U7')\n",
      " array(['neutral', 'neutral'], dtype='<U7')\n",
      " array(['neutral', 'anger'], dtype='<U7')\n",
      " array(['neutral', 'neutral'], dtype='<U7')]\n"
     ]
    }
   ],
   "source": [
    "print(tot_labels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balaji\n"
     ]
    }
   ],
   "source": [
    "v = ['ashwin','arun','balaji','arun']\n",
    "print(max(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f86bd2566df633de3433ebe1e2d10776a9b4d657b73edceb2d78e2ba4453e4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
